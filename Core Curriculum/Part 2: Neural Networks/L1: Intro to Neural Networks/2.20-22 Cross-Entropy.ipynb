{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.20-22 Cross-Entropy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAue+/QjHAITCjtvQ4gxqM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bpRUMCyROASW","colab_type":"text"},"source":["# Cross-Entropy\n","\n","Cross-Entropy is a known method also in Statistics and Machine Learning. \n","The thought process for the decision to use it goes back from the use of Probability.\n","\n","(1) Probability -> We want a Maximum Likelihood \n","\n","Problem: Products are expensive\n","\n","(2) Solution: Use sums via Log Function since log(ab) = log a + log b\n","Problem: log gave out negative numbers\n","\n","Solution:Negate it\n","\n","**Resulting Interpretation:**\n","\n","connection between probability (max likelihood) and error (min error) which is Cross-Entropy\n","- The higher the cross entropy, the higher the error, the lower the probability (vice versa) \n","\n","Cross-Entropy formula:\n","$$CrossEntropy = -[ \\sum_{}^{} y_{i} ln(P_{i}) + (1-y_{i}) ln(1-P_{i}) ]$$"]},{"cell_type":"code","metadata":{"id":"z1UhGS8FNi4P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8d21d852-9647-4bb1-93b3-96145a4a5d4b","executionInfo":{"status":"ok","timestamp":1585393998875,"user_tz":-480,"elapsed":1090,"user":{"displayName":"Lois Anne Leal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwXztWvDI1c1tiyy6OV-AgJspBeOs3YEjH1mV-mg=s64","userId":"13439450328348295859"}}},"source":["import numpy as np\n","\n","# Write a function that takes as input two lists Y, P,\n","# and returns the float corresponding to their cross-entropy.\n","def cross_entropy(Y, P):\n","    CE = np.sum([(-1*(Y[i]*np.log(P[i]) + (1-Y[i])*np.log(1-P[i]))) for i in range(0,len(Y))])\n","    return CE"],"execution_count":3,"outputs":[{"output_type":"stream","text":["ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C84tDpMPNqnt","colab_type":"code","outputId":"5e77fa57-4010-470a-b959-e6260407ef79","executionInfo":{"status":"ok","timestamp":1585394002977,"user_tz":-480,"elapsed":1028,"user":{"displayName":"Lois Anne Leal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgwXztWvDI1c1tiyy6OV-AgJspBeOs3YEjH1mV-mg=s64","userId":"13439450328348295859"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cross_entropy(Y = [1,0,1,1], P = [0.4, 0.6, 0.1, 0.5])"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.828313737302301"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"i5x5lxlnNtqe","colab_type":"text"},"source":["Trying for Y=[1,0,1,1] and P=[0.4,0.6,0.1,0.5].\n","\n","The correct answer is\n","4.8283137373\n","And your code returned\n","4.8283137373\n","\n","Correct!"]},{"cell_type":"markdown","metadata":{"id":"BRYzZg7f2x2X","colab_type":"text"},"source":["## Multi-Class Cross-Entropy\n","\n","The idea is still the same and we can further extend the binary Cross Entropy as:\n","\n","$$MultiClassCrossEntropy = -[  \\sum_{i=1}^{n}\\sum_{j=1}^{m} y_{ij} ln(P_{ij})]$$"]}]}