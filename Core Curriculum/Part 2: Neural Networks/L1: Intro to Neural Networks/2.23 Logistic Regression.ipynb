{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.23 Logistic Regression.ipynb","provenance":[],"authorship_tag":"ABX9TyN1oDkqvg9BKYIMgku9u4ES"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WPtbL98f82pF","colab_type":"text"},"source":["# Logistic Regression\n","Now, we're finally ready for one of the most popular and useful algorithms in Machine Learning, and the building block of all that constitutes Deep Learning. The Logistic Regression Algorithm. And it basically goes like this:\n","\n","Take your data\n","\n","1.   Pick a random model\n","2.   Calculate the error\n","3.   Minimize the error, and obtain a better model\n","4.   Enjoy!"]},{"cell_type":"markdown","metadata":{"id":"DKds7FuWMgci","colab_type":"text"},"source":["## Calculating the Error Function\n"," \n"," Cross-Entropy actually is one of the error functions. \n"," Defining it further here, we add getting average.\n","\n"," $$ ErrorFunction = -\\frac{1}{m} \\sum_{i=1}^{m} (1-y_{i})(ln(1-\\hat{y})) + y_iln(\\hat{y_i})$$\n","\n"," $$expanding: \\hat{y_i} = \\sigma(Wx^{(i)} + b)$$\n"," $$ E(W,b) = -\\frac{1}{m} \\sum_{i=1}^{m} (1-y_{i})(ln(1-\\sigma(Wx^{(i)} + b))) + y_iln(\\sigma(Wx^{(i)} + b))$$\n"]},{"cell_type":"markdown","metadata":{"id":"4XnwCzguSP4L","colab_type":"text"},"source":["### Error Function for Multi-Class\n","\n","We can further extend it as that of the Cross-Entropy:\n"," $$ ErrorFunction = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} y_{ij}ln(\\hat{y_{ij}})$$\n","\n"," $$expanding: \\hat{y_{ij}} = \\sigma(Wx^{(ij)} + b)$$\n"," $$ E(W,b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} y_{ij}ln(\\sigma(Wx^{(ij)} + b))$$\n"]},{"cell_type":"markdown","metadata":{"id":"2G8HuHIsOJ_p","colab_type":"text"},"source":["## Minimizing the Error Function via Gradient Descent\n","\n","In a nutshell, Gradient Descent is one of the algorithms used to minimize the selected error function.\n","![Gradient Descent 1](https://github.com/lbleal1/Deep-Learning-Nanodegree/blob/master/assets/Core%20Curriculum/Part%202:%20Neural%20Networks/Lesson%201:%20Introduction%20to%20Neural%20Networks/rationale-1.png?raw=true)\n","\n","The Gradient Descent Algorithm works as seen below. Note how our old notions of weights and bias, the error function, activation function(this case, the Softmax Function) and the learning rate were used or embedded in Gradient Descent.\n","![Gradient Descent 2](https://github.com/lbleal1/Deep-Learning-Nanodegree/blob/master/assets/Core%20Curriculum/Part%202:%20Neural%20Networks/Lesson%201:%20Introduction%20to%20Neural%20Networks/rationale-2.png?raw=true)"]},{"cell_type":"markdown","metadata":{"id":"HkSUmiqgZiin","colab_type":"text"},"source":["![Gradient Descent Algorithm](https://github.com/lbleal1/Deep-Learning-Nanodegree/blob/master/assets/Core%20Curriculum/Part%202:%20Neural%20Networks/Lesson%201:%20Introduction%20to%20Neural%20Networks/Grad-Descent-Algo.png?raw=true)"]}]}